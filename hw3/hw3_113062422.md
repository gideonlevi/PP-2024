# HW3: All-Pairs Shortest Paths

## Name: 鄔培勇
## Student ID: 113062422


## 1. Implementation

### a. Which algorithm do you choose in hw3-1?
- I used the Blocked Floyd-Warshall algorithm just like in the provided sequential code, which computes the all-pairs shortest paths in a graph using block decomposition. To parallelize the computation, I utilized OpenMP with `#pragma omp parallel` and `#pragma omp simd`.
- I chose to implement this algorithm because it offers several advantages, particularly for large graphs. The block decomposition improves cache locality and reduces memory access overhead. Moreover, the optimization techniques applied to this version can also be adapted to the GPU versions.

### b. How do you divide your data in hw3-2, hw3-3?
- In hw3-2, the data (distance matrix) is divided into blocks of size `B`x`B` (64x64 based on performance tuning). This division allows the Floyd-Warshall algorithm to process one block at a time, focusing on different phases described in the homework spec.

- In hw3-3, in addition to dividing the workload into blocks, the blocks are also divided into half along x-axis so that each GPU only responsible for storing and processing one-half of the blocks and handling specific rounds of computation. During execution, when a particular round is processed, the GPU responsible for that round shares the pivot block with the other GPU. To simplify communication, all blocks along the pivot row are transmitted between GPUs in my implementation. After this, each GPU can execute the phases independently for a round.
    

### c. What's your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor, #blocks, #threads)
- Number of threads per block: 32 x 32
    - This summing up to 1024 threads per block, which utilizes the maximum threads per block supported by GTX 1080 GPU.

- Blocking Factor: B = 64
    - I selected a blocking factor of 64 because it is divisible by 32, which optimizes memory access patterns. A larger value like 64 helps saturate the shared memory and increases computational intensity.

- Number of blocks:
    - For Phase 1 kernel, it only needs one block to be launched to process the pivot block.
    - For Phase 2 kernels, `ceil(n/B)` blocks are launched to process the blocks on pivot row and pivot column.
    - For Phase 3 kernel, `ceil(n/B)`x`ceil(n/B)` blocks are launched to handle the remaining blocks.

### d. How do you implement the communication in hw3-3?
- I utilized Peer-to-Peer (P2P) communication between GPUs for efficient data sharing:
    - First I enabled P2P using `cudaDeviceEnablePeerAccess` to allow direct GPU-to-GPU data transfers, minimizing host memory involvement.
    - The pivot block computed in Phase 1 is shared between GPUs using `cudaMemcpyPeer`, ensuring consistency for subsequent phases. I used OpenMP barriers (#pragma omp barrier) to synchronize GPUs to ensure pivot sharing is complete before moving to the next phase.
    - After all the rounds are executed, each GPU copies its processed data back to the host using `cudaMemcpy`.

### e. Briefly describe your implementations in diagrams, figures or sentences.
![](https://imgur.com/DvJquIm.png)
![](https://imgur.com/kegc8lm.png)
- Phase 1: Computes the shortest paths within the pivot block (the diagonal block corresponding to the current round).
- Phase 2: Updates either the rows or columns intersecting the pivot block.
- Phase 3: Updates all remaining blocks (those not in the pivot's row or column) using the results from phase2_col and phase2_row.

#### hw3-1:
![](https://imgur.com/MHd7K7z.png)
- As mentioned above, I modify the provided hw3-1 sequential vode to add parallelization and simd optimization with `#pragma omp parallel` and `#pragma omp simd`.
- Additionaly, I precompute block boundaries (`block_internal_start_x`, `block_internal_end_x`, etc.) before entering loops, to reduce redundant calculations.
- I also replaced the original conditional update with `Dist[i][j] = l * (l < r) + r * (l >= r)`, where `l = IK + Dist[k][j]` and `r = Dist[i][j]`. This minimizes branching and further optimizes simd performance.

#### hw3-2:
- Memory Management:
    - Pinned the `hDist` memory on CPU
    - Allocated memory on GPU using CUDA
    - Copied distance matrix from CPU to GPU's global memory (`hDist` to `dDist`)
    - Pinning CPU memory during data transfer helps accelerate memcpy operations
- The `block_FW` function consists of a main loop that executes three phases for the specified number of rounds. For each phase, a kernel is created, each with different configuration of number blocks. Phase breakdown:
    - Phase 1: Updates the diagonal block corresponding to the current iteration using shared memory.
    - Phase 2: Updates the row and column blocks of the diagonal block. Two separate kernels (`phase2_row` and `phase2_col`) are used for pivot row and pivot column updates.
    - Phase 3: Updates all remaining blocks using shared memory using previously computed blocks.
- Quadrant Processing:
    - While the maximum number of threads in a block on GTX 1080 GPU is 1024 (32x32), the distance matrix are decomposed into blocks with size 64x64, which requires each thread block to handle four quadrant points in a block.
    - These four quadrant points are processed with indices:
        - (i, j)
        - (i, j + 32)
        - (i + 32, j)
        - (i + 32, j + 32)

- Phase Implementation Details:
    - Copy global memory dDist values to shared-memory
    - Update four quadrant points in block(s).
    - Copy computed results from shared memory back to global memory dDist

#### hw3-3:
![](https://imgur.com/D9mGHyZ.png)
- The multi-GPU implementation maintains most of the core logic from the single-GPU version, with key modifications to handle distributed processing.
- I implemented thread parallelization using openmp, and created parallel threads as many as the available amount of GPUs (2 in this case).
- As mentioned above, the distance matrix data is first distributed evenly on 2 GPUs global memory before entering the main rounds loop.
- Each phase implementation remains unchanged from previous single-GPU version, except for phase 3, which requires block row index updates.
    - Due to GPU receiving data as memorybase + half matrix length.
    - Block row indices need to be adjusted accordingly to account for the offset.
- After all rounds are executed on both GPUs, both of them transfer results back to CPU memory.


## 2. Profiling Results (hw3-2)
Below are the profiling results of `p11k1` testcase with nvprof. The biggest kernel of my program is the `phase3` kernel as it occupied the GPU for the longest time.

![](https://imgur.com/ruShNmI.png)

### Phase 1
![](https://imgur.com/ZOCjzt7.png)

### Phase 2
![](https://imgur.com/TjvVDgs.png)
![](https://imgur.com/vlo2fhL.png)

### Phase 3
![](https://imgur.com/uJkBjUg.png)


## 3. Experiment & Analysis

### a. System Specifications: 
The experiment results are all run on the provided apollo.cs.nthu.edu.tw and apollo-gpu.cs.nthu.edu.tw clusters.

### b. Blocking Factor (hw3-2)
I evaluated the performance of `phase3` kernel using the `p18k1` testcase provided in the homework. The study involved experimenting with different blocking factor configurations and thread block dimensions to optimize the GPU kernel execution.

**Initial Configuration**: `block_dim = (B, B)`
- The thread block dimensions were set to `(blocking factor, blocking factor)`. This approach constrained the blocking factor (B) to the maximum thread block dimensions supported by the GPU, which is `(32, 32)` in this case.
![](https://imgur.com/uzQvaGm.png)

**Optimized Configuration**: `block_dim = (B/2, B/2)`
- To better utilize shared memory and overcome the `(32, 32)` limitation, the thread block dimensions were reconfigured to `(blocking factor / 2, blocking factor / 2)`. This adjustment effectively allowed the testing of larger blocking factors.
![](https://imgur.com/pNaImjo.png)

**Analysis and Observations**:
- Impact of Larger Blocking Factors:
    - The optimized configuration with block_dim = `(B/2, B/2)` allowed testing of larger blocking factors `(B = 64)`, which was not possible with the initial configuration.
    - Larger blocking factors showed better utilization of shared memory, leading to improved performance in most cases.
- GPU Resource Utilization:
    - The initial `(B, B)` configuration was limited by hardware constraints, restricting shared memory usage and computational efficiency.
    - The `(B/2, B/2)` configuration balanced thread distribution and shared memory usage, providing a more scalable approach.
- Performance Trends:
    - Results from the plots suggest that shared memory optimization has a significant impact on kernel execution time, particularly when larger blocking factors are employed.

---

### c. Optimization (hw3-2)

- Coalesced memory access
    - Memory accesses are optimized for GPU global memory by ensuring threads in a warp access consecutive memory locations. This improves bandwidth utilization by reducing memory transactions.
    - For example, in the `phase1` kernel, shared memory is used to load blocks from global memory in a coalesced manner before performing computations.

- Shared memory
    - Shared memory is heavily used to reduce global memory access latency. Each thread block loads data into shared memory (e.g., `shr_pivot`, `pivot_row`, `pivot_col`, and `shared_mem` arrays), performs calculations locally, and writes the results back to global memory.
    - This minimizes repeated global memory accesses, which are costly in terms of latency.

- Distance matrix padding
    - The matrix size n is padded to the nearest multiple of the blocking factor (`B`) by computing `ceil_n = ceil(n, B) * B`. This ensures all blocks are uniform in size and eliminates boundary conditions during computation. Padding simplifies memory access patterns and enables efficient block-level operations.

- Occupancy optimization
    - As mentioned above, the grid and block dimensions (`dim3 grid_dim(blocks, blocks)` and `dim3 block_dim(half_B, half_B)`) are chosen to maximize occupancy of shared memory.
    - To implement this optimization, the kernel of each phases have to be modified since each thread block have to handle four quadrant points in a block.

- Large blocking factor (`B = 64`)
    - A relatively large blocking factor is chosen to increase the amount of work done per block and maximize shared memory reuse.

- Kernel disaggregation
    - The `phase2` kernel was divided into two separate kernels (`phase2_row` and `phase2_col`) because the operations on the pivot row and column are independent of each other. This separation helps mitigate the risk of shared memory overflow when handling larger datasets.
    - While the separation avoids potential shared memory constraints, the performance improvement is not substantial. This is likely due to the overhead introduced by launching multiple kernels, which offsets the gains from shared memory optimization.
    - The `phase1` and `phase3` kernels are not separated because they process only one block at a time and have dependencies between the pivot row and column. Separating them would introduce synchronization challenges and potentially degrade performance due to increased kernel-launch overhead.

- Loop unrolling
    - The `#pragma unroll 4` directive in the kernels allows the compiler to unroll loops over the blocking factor (`B`). This optimization reduces loop overhead and enables better instruction-level parallelism within the GPU.

![](https://imgur.com/peFxdSR.png)

---

### d. Weak Scalability (hw3-3)

#### **p12k1 vs. p17k1 (Single GPU vs. 2 GPUs)**
- The workload was chosen because the squared number of vertices (V^2) for `p17k1` is approximately double that of `p12k1`, aligning with the weak scalability principle.
- Calculations confirmed that (17024^2 / 11840^2 ≈ 2.067), which closely matches the theoretical expectation.
- However, the runtime comparison revealed some deviation from perfect scalability, likely due to the overhead introduced by GPU communication and load balancing when distributing tasks across two GPUs.
![](https://imgur.com/b2ocUIP.png)

#### **p12k1 vs. p16k1 (Single GPU vs. 2 GPUs)**
- Unlike `p17k1`, `p16k1` was selected based on the runtime similarity to `p12k1` on a single GPU, as the presence of edges (E) complicates workload prediction.
- Despite this, (V^2) for `p16k1` (16000^2) is roughly double that of `p12k1` (11840^2), with (16000^2 / 11840^2 ≈ 1.826), supporting the weak scalability assumption.
- The runtime scaling was closer to expectations compared to `p17k1`, suggesting that the choice of test cases impacts the accuracy of weak scalability measurements.
![](https://imgur.com/c7q9JNK.png)

#### **Insights**
- Weak scalability is sensitive to both vertex and edge counts, as (E) significantly affects computational workload beyond what (V^2) alone predicts.
- Communication overhead and load imbalance between GPUs limit the scalability, especially when edge distribution is uneven.

---

### e. Time Distribution (hw3-2)

#### Analysis of Time Spent in Computing, Memory Copy, and I/O with Respect to Input Size

#### NVIDIA GPU
- **Computing Time**: The computation time increases with the input size, which indicates that the computational workload scales efficiently with input size.
- **Memory Copy Time**: The memory copy time also grows with input size but at a slower rate than the computation time. This suggests that memory bandwidth is not a significant bottleneck compared to computation.
- **I/O Time**: I/O time also increases linearyly with the input size.

![NVIDIA GPU Computing Time](https://imgur.com/CT77diC.png)

![NVIDIA GPU Memory and I/O Time](https://imgur.com/cVUyeiW.png)


#### AMD GPU
- **Computing Time**: Similar to the NVIDIA GPU, computing time scales along with input size. However, the slope is slightly different, indicating variations in processing efficiency.
- **Memory Copy Time**: Memory copy time grows consistently with input size. Compared to the NVIDIA GPU, AMD GPUs show a comparable trend, though absolute times may differ.
- **I/O Time**: I/O time also increases linearyly with the input size. The I/O time here are roughly similar to the one with NVIDIA GPU, but the proportion seems to be bigger because computation time on AMD GPU is a lot faster than NVIDIA GPU.

![AMD GPU Computing Time](https://imgur.com/HpmX72f.png)

![AMD GPU Memory and I/O Time](https://imgur.com/DcrpIEO.png)

#### Key Observations

1. Both NVIDIA and AMD GPUs exhibit increasing scaling in computation time relative to input size.
2. Memory copy time shows a steady increase with input size, but the rate of growth is manageable compared to computational scaling.
3. I/O time exhibit linear scaling relative to input size.
4. Differences in computation and memory performance between NVIDIA and AMD GPUs highlight hardware-specific characteristics that can influence workload distribution and optimization strategies.

---

### Others
#### Bank Conflict Test

- I profiled the execution of `hw3-2` on several test cases using `nvprof` to check for bank conflicts. The profiling command used was: `srun -p nvidia -N1 -n1 --gres=gpu:1 nvprof --events shared_ld_bank_conflict,shared_st_bank_conflict ./hw3-2 ./testcases/p12k1 ./p12k1.out`
- The results indicated no bank conflicts, as shown in the output below. Therefore, I did not implement optimizations like shared memory padding or similar techniques.

![](https://imgur.com/4d7JGuG.png)

#### Variable caching and Shift operators
- Caching repeatedly used values like multiplication result in a variable can reduce the computation time. Using bitwise operations is also a lot more efficient than standard operations, e.g. `<<` compared to `*`.

#### Minimize Branches in GPU Code:
- Avoid constructs like `if` or `switch` on GPUs, as they disrupt parallelization and diminish performance (warp divergence). The fewer branches, the better the GPU's parallel workload distribution.

#### Loop Unrolling:
- Unrolling loops by smaller number provides better performance than by larger number in this case. In my code I find that unrolling loop by 4 works the best.
- This is likely because unrolling by larger number can lead to underutilized processing time due to incomplete parallelization. Unrolling by smaller number aligns better with warp design and maintains consistency. Unrolling a loop increases the size of the binary, leading to a phenomenon called code bloat. If the increased code size exceeds the capacity of the instruction cache (I-cache), the program may experience more cache misses, slowing execution.

#### Use Built-In Functions Over Ternary Operators:
- Built-in functions, such as min or max, are often faster than implementing the same logic with ternary operators. Even though ternary operators avoid branches, built-in functions are optimized for performance.

#### Peer-To-Peer Communication
- In my initial approach for `hw3-3`, I facilitated data transfer between GPUs by first sending it back to the CPU and subsequently distributing it to the GPUs through pre-prepared threads. However, this method appeared inefficient due to the additional overhead. To optimize this process, I implemented peer-to-peer (P2P) communication, allowing GPUs to exchange data directly without involving CPU memory. 
- Surprisingly, the performance gains from this mechanism change were insignificant. My guess is that the limited improvement comes from the lack of direct interconnect support between GPUs like PCIe or NVLink, in the current setup. As a result, GPU-to-GPU communication may still be routed through the CPU, which diminishes the benefits of P2P transfers.

## 4. Experiment on AMD GPU
- Single GPU
![](https://imgur.com/Ebk9Ooe.png)
![](https://imgur.com/6KTP17O.png)

- Multi-GPU
![](https://imgur.com/qI1XPX8.png)
![](https://imgur.com/kRXD7G4.png)

### Observations and Insights
#### Single GPU Performance
- The performance graphs indicate a steady speedup on the AMD GPU compared to the NVIDIA GPU across various workloads in the single GPU configuration. This suggests that the AMD GPU is more efficient in terms of computation and bandwidth for the tested scenarios.
- The AMD GPU demonstrates consistent performance improvements even under increasing workloads, which highlights its capability to handle scalable tasks effectively. In contrast, the NVIDIA GPU is slower, possibly due to architectural bottlenecks or less optimized utilization of bandwidth.

#### Multi-GPU Performance
- Similar to the single GPU configuration, the AMD GPUs in the multi-GPU setup show better scalability and performance compared to NVIDIA GPUs. The performance gains are more pronounced as the workload size increases, suggesting effective inter-GPU communication and workload distribution in the AMD architecture.
- The NVIDIA GPUs, while functional in multi-GPU configurations, appear to suffer from diminishing returns as the workload scales, which could be attributed to interconnect latency or less efficient scaling mechanisms.

## 5. Experience & Conclusion
- This time, I learned how to allocate data and memory effectively on the GPU, and also gained an understanding of hardware limitations and design architecture, such as warp, blocks, and the maximum number of threads per block on a GPU.

- It's clear that with a solid understanding of the hardware background, optimizing memory allocation and distribution in software can significantly improve the entire process. The bottleneck identified in this experiment is clearly related to memory access, optimizing this aspect would likely be the most effective first step.

- Some optimization methods were previously unknown to me, such as compiler flags, pragma unroll, and CUDA APIs. After seeing how others implemented these techniques, I recognized their value and directly applied them to my code.

- The difficulty of this assignment was relatively high. While the implementation steps were straightforward, proper memory allocation and data transfer on the GPU were critical. For example, improper space allocation led to issues like accessing incorrect locations or missing values. Given the assignment’s iterative nature, debugging and tracing errors are also challenging. Additionally, the optimization possibilities are vast, as there are numerous strategies to accelerate computation on the GPU, each with its own trade-offs.
