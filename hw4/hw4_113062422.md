# HW4: FlashAttention

## Name: ÈÑîÂüπÂãá
## Student ID: 113062422

---

## 1. Implementation
### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (‚Ñì and ùëö) were calculated.
- In implementing the FlashAttention forward pass using CUDA, I divided the matrix operations into smaller blocks to improve parallelism and utilize shared memory (SRAM) effectively. I used a block-based approach with a grid of threads, where each block computes a portion of the attention mechanism. Specifically, the algorithm is structured as follows:
    - **Matrix Blocking**: The attention computation is broken into smaller tiles, with the batch dimension divided by the block size (br for rows, bc for columns). Each block processes a portion of the query (Q), key (K), and value (V) matrices.
    - **SRAM Usage**: The blocks utilize shared memory to store sections of Q, K, and V. Each thread in the block loads part of the matrices into the shared memory to avoid redundant global memory accesses. This step accelerates the dot product and scaling operations
    - **Scaling Factors (‚Ñì and ùëö)**: The maximum value (ùëö) for each row is calculated by comparing each element in the shared memory and retaining the largest value. The scaling factor (‚Ñì) is computed by first exponentiating the difference between each element and the maximum, followed by summing the results for the entire row.
    - **Dot Product & Attention Computation**: After scaling, I computed the dot product of Q and K, applied softmax, and then computed the final output by taking the weighted sum of V using the attention weights. The results are stored back in the output matrix (O).
    - **Intermediates**: The updated values of ‚Ñì and ùëö are computed after each iteration and stored back in memory for subsequent iterations.
- This design allows efficient memory access and parallel computation, ensuring optimal performance while leveraging shared memory to minimize global memory bandwidth usage.

### b. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
![](https://imgur.com/EbkNW1q.png)
- The division is as follows:
    - br (block rows): Defines how many rows each block will process. This is typically a small value to make the block size manageable in terms of shared memory.
    - bc (block columns): Defines how many columns each block will process, which is also kept small for efficiency. In this implementation I used 32 for both br and bc, as it is the maximum number of threads in a thread block for the provided GPU.
    - tr (tiles of rows): The number of row tiles (tr = ceil(N / br)).
    - tc (tiles of columns): The number of column tiles (tc = ceil(N / bc)).
- Each thread block processes a small portion of the matrix by focusing on a subset of rows and columns. The total number of blocks will be (B, tr), where B is the batch size.
- **Thread Block Division**:
    - Each block is assigned to compute a portion of the attention operation for one batch. The block itself is further divided into threads arranged in a 2D grid, with each thread handling a specific row of matrix blocks in a batch.
    - The block is organized as a 2D grid of threads with dimensions (br, bc). Each thread in the block handles a specific position in the matrix. The threads cooperatively load parts of the input matrices (Q, K, and V) into shared memory and compute the attention values.

![](https://imgur.com/HsdUpJL.png)

- **Dot Product Computation**:
    - After loading the data into shared memory, threads within the block compute the dot products of the query and key vectors.
    - The dot product is calculated for each pair of Q and K values, which is then scaled by the scalar factor (1 / sqrt(d)), where d is the dimension of each query and key vector.
- **Softmax and Scaling**:
    - The results of the dot products (before softmax) are stored in shared memory (shared_s). The threads within a block compute the softmax by first finding the maximum value for each row (to avoid overflow during exponentiation), then subtracting it from each element, applying the exponential function, and normalizing by the row sum.
- **Weighted Sum (Output Computation)**:
    - After computing the softmax, the threads perform a weighted sum of the V matrix values, based on the attention weights. Each thread computes its corresponding output value using the attention scores and the V values. 
    - The result of this operation is written back to the output matrix O.
- **Handling Multiple Batches**:
    - The computation is done in parallel across multiple batches (B). Each batch is processed independently, which is handled by the grid_dim.x = B setting in the kernel launch configuration. Each batch processes its corresponding row tile of the matrices in parallel, ensuring that the overall computation is highly parallelized across both batches and matrix row tiles.
- **Final Result**:
    - After the kernel completes its execution, the computed outputs are copied back to the host memory, and the intermediate matrices (l and m) are freed from the device memory.


### c. Describe how you chose the block sizes B_r‚Äã and B_c‚Äã and why.
- **Thread Occupancy**:
    - The goal is to maximize the number of threads that can be executed in parallel on the GPU. To do this efficiently, the block size must be chosen so that the GPU‚Äôs hardware resources, such as registers and execution units, are fully utilized. A smaller block size might lead to underutilization of available threads.
    - Because the maximum threads that can be in a thread block is 1024 threads or 32x32 threads, I pick br and bc to also be 32 each to maintain simplicity of the code. Picking a block size different with block threads size will cause an additional complexity as each kernel also have to take account of remainders.


### d. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
- **Number of Threads per Block**:
    - br = 32: Number of rows in each block. Each thread block will process a block of br rows from the matrix.
    - bc = 32: Number of columns in each block. Each thread block will process a block of bc columns from the matrix.
    - Therefore, each block will contain br * bc = 32 * 32 = 1024 threads, which is the maximum number of threads supported in the provided NVIDIA GTX 1080 GPU.
- **Shared Memory Allocation**:
    - Shared memory is used within each block to store slices of the matrices Q, K, and V, as well as intermediate results for the attention computation (e.g., the attention scores and the scaling factors). The amount of shared memory required per block depends on the block size (br, bc) and the dimensions of the matrices.
    - shared_q: Size = br * d (size for Q values)
    - shared_k: Size = br * bc (size for K values)
    - shared_v: Size = bc * d (size for V values)
    - shared_s: Size = br * bc (size for the attention scores)
    - shared_l: Size = br (size for rows' l)
    - shared_m: Size = br (size for rows' m)
- **Grid Dimensions**:
    - The x-dimension of the grid corresponds to the batch size B. Each batch processes its own set of blocks independently.
    - The y-dimension of the grid corresponds to the number of row tiles (tr), which is determined by dividing the sequence length N into blocks of size br.
    - Therefore, the total number of blocks launched is B * tr.


### e. Justify your choices and how they relate to the blocking factors and the SRAM size.
- **Blocking factor**:
    - **Matrix Dimensions**: The matrices Q, K, and V are divided into blocks of size br (rows per block) and bc (columns per block). This division enables parallel processing by assigning a block of work (submatrix) to each thread block on the GPU.
    - **Tile Dimensions** (br and bc): The blocking factors, br = 32 and bc = 32, are chosen as they strike a balance between memory usage, computation load, and parallelism. These sizes are typical for matrix operations in CUDA because they fit well with the GPU architecture, especially when working with shared memory.
- **Shared Memory**:
    - **Memory Constraints**: The shared memory available on a GPU‚Äôs streaming multiprocessor (SM) is limited (e.g., 48 KB or 96 KB, depending on the GPU model). Each thread block in the kernel must load relevant portions of the matrices Q, K, V, and intermediate data (like the attention scores) into shared memory. Thus, the block size needs to be chosen such that the total shared memory used by each block does not exceed the available shared memory.

- **Summary**:
    - **Block Size** (br = 32, bc = 32): These sizes are chosen to balance shared memory usage and parallelism, fitting within the GPU‚Äôs shared memory limits while ensuring efficient computation. The block size ensures high thread occupancy and coalesced memory access, leading to optimal performance.
    - **Grid Size**: The grid dimensions are configured to process the batch size and sequence length in parallel, ensuring that each batch is processed independently and that all rows of the sequence are covered by the grid of blocks.
    - **Shared Memory Usage**: The block size ensures that the shared memory usage per block is within the GPU‚Äôs limits, allowing for efficient data reuse and minimizing global memory access.
    - **Scalability**: The configuration allows for scalability with respect to batch size and sequence length, ensuring that the implementation can handle large datasets efficiently.


## 2. Profiling Results
Below are the profiling results of the provided `t20` testcase with nvprof.

###
![](https://imgur.com/D8GeHvu.png)

---

## 3. Experiment & Analysis

### a. System Specifications: 
The experiment results are all run on the provided apollo-gpu.cs.nthu.edu.tw cluster.

### b. Optimizations
- **Coalesced memory access**:
    - To improve memory access efficiency, I ensured that threads within a warp access contiguous memory locations in a coalesced manner. This optimization reduces the number of memory transactions by grouping memory accesses efficiently, thus improving global memory throughput and minimizing access latency.

- **Shared memory**:
    - I used shared memory to store frequently accessed data that is shared among threads within the same block. This reduces the need for global memory accesses, which are slower. In my implementation, shared memory stores parts of the query, key, and value matrices for each tile, allowing threads to access this data locally and efficiently. 
    - I also use shared memory for l and m to reduce read and write from/to HBM. Since all threads in the same block will process the same row of a matrix block, sharing this l and m array for threads in the same block is sufficient. This approach can reduce the overheads for memory allocation and memory transfer between host and device, which is beneficial to perforamnce.

- **Sequence parallelism**:
    - By breaking down tasks into smaller sub-tasks, which is matrix row tile in this case, I enabled multiple threads to process them concurrently. This is particularly useful in the kernel where threads compute dot products, matrix multiplications, and other operations on different parts of the data simultaneously, increasing overall throughput.

- **Increase thread block dimension**:
    - Originally I only use 1 dimension of 32 threads as my block thread dimensione. Later I increased the thread block dimensions (set to 32 for both rows and columns) to better utilize the GPU‚Äôs compute resources. This allows more threads to collaborate in the computation of the flash attention mechanism, which enhances parallelism and ensures better resource utilization across the GPU.

- **Bank conflict reduction with accumulator variable**
    - I profiled the execution of my implementation on several test cases using `nvprof` to check for bank conflicts. The profiling command used was: `srun -p nvidia -N1 -n1 --gres=gpu:1 nvprof --events shared_ld_bank_conflict,shared_st_bank_conflict ./hw4 /home/pp24/share/hw4/testcases/t20 ./t20.out`
    - The profiling results revealed some occurence of bank conflicts, as shown in the figures below. While the current implementation performs relatively well, these results also suggest opportunities for optimization. Specifically, one potential improvement is the use of array padding to mitigate bank conflicts and further enhance memory access efficiency.

    ![](https://imgur.com/HI7V864.png)
    ![](https://imgur.com/uEL8ERU.png)
    - From this result, I observed that my code contains some part that accesses shared memory indices that is accessed by a lot of threads too much. For example:
    ``` c
    for (int x = 0; x < d_per_bc; x++) {
        // Load kj and vj into shared memory
        // partition kj into d_per_bc partitions
        shared_k[(tid_y * bc) + tid_x] = k[qkv_batch_offset + (tile_size * j) + (tid_y * d) + (x * bc) + tid_x]; // kj
        shared_v[(tid_y * d) + (x * bc) + tid_x] = v[qkv_batch_offset + (tile_size * j) + (tid_y * d) + (x * bc) + tid_x]; // vj
        __syncthreads();
        
        // q dot k and scalar
        #pragma unroll 32
        for (int y = 0; y < bc; y++) {
            shared_s[(tid_y * bc) + tid_x] += shared_q[(tid_y * d) + (x * bc) + y] * shared_k[(tid_x * bc) + y];
        }
        __syncthreads();
    }
    shared_s[(tid_y * bc) + tid_x] *= scalar;
    ```
    - Here i write to `shared_s[(tid_y * bc) + tid_x]` in a loop, which is also accessed by every other threads in the same block. To avoid this, I used an accumulator variable to hold the sum of calculations first before writing the final result to the shared memory location.
    ``` c
    float S_ij = 0.f;
    for (int x = 0; x < d_per_bc; x++) {
        // Load kj and vj into shared memory
        // partition kj into d_per_bc partitions
        shared_k[(tid_y * bc) + tid_x] = k[qkv_batch_offset + (tile_size * j) + (tid_y * d) + (x * bc) + tid_x]; // kj
        shared_v[(tid_y * d) + (x * bc) + tid_x] = v[qkv_batch_offset + (tile_size * j) + (tid_y * d) + (x * bc) + tid_x]; // vj
        __syncthreads();

        // q dot k and scalar
        #pragma unroll 32
        for (int y = 0; y < bc; y++) {
            S_ij += shared_q[(tid_y * d) + (x * bc) + y] * shared_k[(tid_x * bc) + y];
        }
        __syncthreads();
    }
    shared_s[(tid_y * bc) + tid_x] += S_ij * scalar;
    ```
    - With this, I can reduce the amount of shared memory bank conflicts, as shown below:

    ![](https://imgur.com/QjCLMYo.png)

    - But still, there are unavoidable shared memory bank conflicts because in the calculations there are still a lot of threads accessing the same location of the shared memory. For example `S_ij += shared_q[(tid_y * d) + (x * bc) + y] * shared_k[(tid_x * bc) + y];` will leads to bank conflicts in accessing `shared_k`, because all threads with different `tid_y` value in the same block will access the same location.

- **Loop unrolling**:
    - To reduce the overhead of loop control, I manually expanded certain loops, which decreases the number of iterations and allows the compiler to better optimize the computations. In my code, loop unrolling was applied to loops calculating dot products and row sums, which speeds up these operations by reducing iterations and increasing instruction-level parallelism.

- Together, these optimizations contribute to a significant performance boost in the flash attention mechanism by reducing memory bottlenecks, improving data locality, and maximizing the computational resources available on the GPU. The graph below is the speedup of implementing each optimizations to testcase t20.

![](https://imgur.com/CT9BCAz.png)


### c. Potential Optimizations
- **Shared memory occupancy optimization / Larger blocking factor**:
    - In my implementation, shared memory is heavily utilized to hold the query (shared_q), key (shared_k), value (shared_v), attention scores (shared_s), l, and m for the block. However, the current partitioning might not fully optimize the shared memory usage. Because in the spec it is mentioned that the maximum size of dimension (d) is 64, then the maximum usage of shared memory is 2 * (32 * 64 + 32 * 32 + 32) * 4 bytes = 24832 bytes, which is still far from the size of shared memory on the provided GPU (48KB = 49152 bytes). By increasing the block size (i.e., the number of threads per block), the number of active threads per block could be optimized for better memory coalescing, which would improve memory throughput.
    - A larger blocking factor would allow more data to be processed in parallel, reducing the overhead of memory accesses, but care must be taken to not exceed the shared memory limits of the GPU.
    - Tuning the br and bc parameters to balance shared memory usage and thread occupancy could lead to a better fit for different GPU architectures.

- **FlashAttention-2**:
    - FlashAttention-2 is a more advanced version of FlashAttention that implements optimizations for even better memory efficiency and computational throughput. One of the improvements in FlashAttention-2 is that it incorporates more sophisticated tile-based algorithms, which better utilize the GPU's memory hierarchy and computational units. It reduces redundant computations by carefully partitioning the data and minimizing the number of synchronization points in kernel execution. Implementing these advanced techniques could drastically reduce the memory footprint and improve the overall runtime of my implementation.

- **FlashDecoding++'s unified row max**:
    - FlashDecoding++ optimizes the row max computation by unifying it across multiple attention blocks, reducing redundant calculations and synchronization overhead. Instead of computing the row max separately for each block, this technique consolidates the max values across blocks, allowing for more efficient memory access and better parallelism.
    - By implementing this unified approach, my kernel could reduce the need for multiple global memory accesses to update and share max values, leading to faster processing times, especially for large attention matrices. This optimization would enhance memory efficiency and computational throughput, improving overall performance.

---

### d. Others
#### Time Distribution
![](https://imgur.com/A9g99yE.png)
![](https://imgur.com/IQOcTG6.png)

#### FlashAttention Optimization Experiment
``` c
// find max
float prev_row_m = row_m_i;
float row_m = row_m_i;
// #pragma unroll 32
for (int y = 0; y < bc; y++) {
    if (shared_s[(tid_y * bc) + y] > row_m) {
        row_m = shared_s[(tid_y * bc) + y];
    }
}
__syncthreads();

//
for (int x = 0; x < d_per_bc; x++) {
    shared_o[x] *= __expf(prev_row_m - row_m);
}
float row_l = __expf(prev_row_m - row_m) * row_l_i;
__syncthreads();

// compute Pij
float P_ij;
#pragma unroll 32
for (int y = 0; y < bc; y++) {
    P_ij = __expf(shared_s[(tid_y * bc) + y] - row_m); // Pij
    row_l += P_ij; // lij = rowsum(Pij)
    for (int x = 0; x < d_per_bc; x++) {
        shared_o[x] += P_ij * shared_v[(y * d) + (x * bc) + tid_x];
    }
}
row_l_i = row_l;
row_m_i = row_m;

__syncthreads();
```
- In this experiment, I modified the FlashAttention kernel to compute the row-wise maximum (`new_row_m`) earlier and moved the normalization factor (`exp(prev_row_m - row_m)`) outside of the main loop, aiming to reduce the number of renormalization operations. However, the modified version resulted in slower performance due to increased thread dependencies, which led to requiring more frequent thread synchronizations (`__syncthreads`). The original approach, with fewer dependencies and synchronization points, performed better, highlighting the importance of managing thread dependencies effectively in parallel algorithms.

#### Variable caching
- Caching repeatedly used values like multiplication result or addition result in a variable can reduce the computation time.

#### Loop Unrolling:
- Unrolling loops by smaller number provides better performance than by larger number in my case of implementation.
- This is likely because unrolling by larger number can lead to underutilized processing time due to incomplete parallelization. Unrolling by smaller number aligns better with warp design and maintains consistency. Unrolling a loop increases the size of the binary, leading to a phenomenon called code bloat. If the increased code size exceeds the capacity of the instruction cache (I-cache), the program may experience more cache misses, slowing execution.

#### Minimize Branches in GPU Code:
- I always try to avoid constructs like `if` or `switch` on GPUs, as they disrupt parallelization and diminish performance (warp divergence). The fewer branches, the better the GPU's parallel workload distribution.


## 4. Experiences / Conclusion

Through this assignment, I got hands-on experience with CUDA programming while optimizing a flash attention mechanism. I learned a lot about managing memory effectively on both the host and device, optimizing shared memory usage, and the importance of memory coalescing and data locality. Thread synchronization was essential for keeping computations consistent. Debugging GPU kernels was challenging, especially when finding the best parallelism strategies and figuring out the dimensions of different matrices. Fine-tuning memory access patterns, grid/block sizes, and using compiler optimizations helped improve performance. Overall, this strengthened my understanding of GPU architecture and parallel programming.