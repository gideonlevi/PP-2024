# PP HW 5 Report Template
> - Please include both brief and detailed answers.
> - The report should be based on the UCX code.
> - Describe the code using the 'permalink' from [GitHub repository](https://github.com/NTHU-LSALAB/UCX-lsalab).

## 1. Overview
> In conjunction with the UCP architecture mentioned in the lecture, please read [ucp_hello_world.c](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/pp2024/examples/ucp_hello_world.c)
1. Identify how UCP Objects (`ucp_context`, `ucp_worker`, `ucp_ep`) interact through the API, including at least the following functions:
    - `ucp_init`: This function initializes the UCP context (`ucp_context`). It takes user-defined parameters (`ucp_params_t`), configurations (`ucp_config_t`), and pointer to UCP context (`&ucp_context`) as input. This function will help us set up the global context for UCX, including [filling configurations into the context](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/7c0362c97c8fe9cbeaacaac90271dde0210ac529/src/ucp/core/ucp_context.c#L2150), [allocating resources](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/7c0362c97c8fe9cbeaacaac90271dde0210ac529/src/ucp/core/ucp_context.c#L2161) like communication devices, memory registration caches, transport layers, and more. This function must be called before creating workers or endpoints because they depend on the resources managed by the context.
    - `ucp_worker_create`: [This function creates a worker (`ucp_worker`), which represents a communication instance.](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/84e459e73df4f02aecd044c44e4584d88f4b9b0e/src/ucp/core/ucp_worker.c#L2311) It requires a UCP context and worker parameters (`ucp_worker_params_t`) as input, which then uses the context to allocate and configure a worker instance before returning a UCP worker handle. Multiple workers can exist within the same context, but ach worker manages its own communication and can handle multiple endpoints. Workers provide the foundation for creating endpoints (`ucp_ep`), which establish connections remote workers.
    - `ucp_ep_create`: This function establishes a communication endpoints (`ucp_ep`), which connects a local worker (`ucp_worker`) to a remote worker. It takes a worker (`ucp_worker`) and endpoint parameters (`ucp_ep_params_t`) which includes remote worker's address (`ucp_address_t`) as input, and it [creates the endpoint handle (`ucp_ep`) for the communication](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/7c0362c97c8fe9cbeaacaac90271dde0210ac529/src/ucp/core/ucp_ep.c#L1186). Endpoints represent the actual "pipe" that allows data to flow between two workers.
2. UCX abstracts communication into three layers as below. Please provide a diagram illustrating the architectural design of UCX.
    - `ucp_context`: One instance per application process
    - `ucp_worker`: Each instance of UCP context can have multiple workers to progress communications
    - `ucp_ep`: Each instance of worker can also have multiple endpoints for communication peers

    ![](https://imgur.com/qOXMQBa.png)
> Please provide detailed example information in the diagram corresponding to the execution of the command `srun -N 2 ./send_recv.out` or `mpiucx --host HostA:1,HostB:1 ./send_recv.out`
```
[pp24s098@apollo31 mpi]$ srun -N 2 ./send_recv.out
Process 0 sent message 'Hello from rank 0' to process 1
Process 1 received message 'Hello from rank 0' from process 0
```
- `srun -N 2 ./send_recv.out`, This command runs the MPI program on 2 nodes, and each node corresponds to a process which communicate using UCX.
    - `ucp_context`: One global communication context is initialized on each process. This provides shared resources for device discovery and communication setup.
    - `ucp_worker`: Each process creates its own worker. The worker manages asynchronous communication and ensures data transfer.
    - `ucp_ep`: An endpoint is created on the worker to connect to the remote process. Process 0's worker creates an endpoint for Process 1, and Process 1's worker creates an endpoint for Process 0.
    
    ![](https://imgur.com/Ug3G6Xm.png)

3. Based on the description in HW5, where do you think the following information is loaded/created?
    - `UCX_TLS`: Since this information refers to all the available TLS, I would guess that it's loaded during the initialization of the UCP context, possibly in the `ucp_init()` function. This would make sense because UCX needs to gather information about the supported transport layers on the server to fill the context.
    - `TLS selected by UCX`: I think the selected TLS is determined when a worker is created, likely during the `ucp_worker_create()` function. Since each worker manages communication, I assume that UCX selects the appropriate TLS based on the workerâ€™s specific communication needs and available resources at that point.

## 2. Implementation
> Please complete the implementation according to the [spec](https://docs.google.com/document/d/1fmm0TFpLxbDP7neNcbLDn8nhZpqUBi9NGRzWjgxZaPE/edit?usp=sharing)
> Describe how you implemented the two special features of HW5.
1. Which files did you modify, and where did you choose to print Line 1 and Line 2?
-  `src/ucs/config/parser.c`
    - I modified [ucs_config_parser_print_opts()](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/84e459e73df4f02aecd044c44e4584d88f4b9b0e/src/ucs/config/parser.c#L1853) function to print available TLS when `ucp_config_print` is invoked.
    ``` c
    char** envp;
    if (flags & UCS_CONFIG_PRINT_TLS) {
        for (envp = environ; *envp != NULL; ++envp) {
            if (strncmp("UCX_TLS", *envp, 7) == 0) {
                fprintf(stream, "%s\n", *envp);
            }
        }
    }
    ```
    - Inside the flag check block, the code will iterate through process environment variables (`environ`) and print the elements that have prefix `"UCX_TLS"`.
- `str/ucs/config/types.h`
    - I modified [ucs_config_print_flags_t](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/84e459e73df4f02aecd044c44e4584d88f4b9b0e/src/ucs/config/types.h#L94C3-L94C27) to add a flag where we want to print the TLS configuration.
    ``` c
    typedef enum {
        UCS_CONFIG_PRINT_CONFIG          = UCS_BIT(0),
        UCS_CONFIG_PRINT_HEADER          = UCS_BIT(1),
        UCS_CONFIG_PRINT_DOC             = UCS_BIT(2),
        UCS_CONFIG_PRINT_HIDDEN          = UCS_BIT(3),
        UCS_CONFIG_PRINT_COMMENT_DEFAULT = UCS_BIT(4),
        UCS_CONFIG_PRINT_TLS             = UCS_BIT(5)
    } ucs_config_print_flags_t;
    ```
- `src/ucp/core/ucp_worker.c`
    - I modified [ucp_worker_print_used_tls()](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/84e459e73df4f02aecd044c44e4584d88f4b9b0e/src/ucp/core/ucp_worker.c#L1764) function to print available TLS and the used TLS.
    ``` c
    const char* used_tls;
    ucp_config_print(NULL, stdout, NULL, UCS_CONFIG_PRINT_TLS);
    used_tls = ucs_string_buffer_cstr(&strb);
    fprintf(stdout, "%s\n", used_tls);
    ```
    - For the first line I invoke `ucp_config_print()` which will then invoke `ucs_config_print_parser_opts()` to print the available TLS to the stdout stream.
    - For the second line I convert the used TLS information inside the string buffer (`strb`) into a C-style string, and use `fprintf()` to print them out to the stdout stream.

2. How do the functions in these files call each other? Why is it designed this way?
- In `src/ucs/config/parser.c`, the `ucs_config_parser_print_opts()` function is responsible for printing the available TLS options by iterating over the environment variables. `The ucp_config_print()` function calls `ucs_config_print_parser_opts()` to print the available TLS options. This function is invoked by a worker in `src/ucp/core/ucp_worker.c`, which initiates the process of printing the TLS configuration.
- The design choice for this function call hierarchy helps keep the code organized and flexible. By separating the tasks of gathering and printing the available TLS options into a separate module, the code is easier to understand and reuse. It also allows different parts of the process, like printing TLS or configuration details, to be updated or changed independently without affecting the rest of the code.

3. Observe when Line 1 and 2 are printed during the call of which UCP API? 
- It is printed when [ucp_ep_create()](https://github.com/NTHU-LSALAB/UCX-lsalab/blob/84e459e73df4f02aecd044c44e4584d88f4b9b0e/src/ucp/core/ucp_ep.c#L1176) is invoked. It will invoke `ucp_ep_create_to_sock_addr()`, then calls `ucp_ep_init_create_wireup()`, which calls `ucp_worker_get_ep_config()`, and finally calls `ucp_worker_print_used_tls()`.

4. Does it match your expectations for questions **1-3**? Why?
- No, it doesn't match my expectations. It turns out the information is loaded and created when the endpoint is created, rather than when the worker is created. This design choice likely makes sense because a worker can have multiple endpoints, and allowing different TLS configurations for each endpoint provides better flexibility, compatibility, and performance.

5. In implementing the features, we see variables like lanes, tl_rsc, tl_name, tl_device, bitmap, iface, etc., used to store different Layer's protocol information. Please explain what information each of them stores.
- `lanes`: This variable stores information about the communication lanes used for data transfer between different layers. It helps manage the different types of transport available for a specific protocol.
- `tl_rsc`: This represents the transport layer resource, storing details about the specific transport resource being used for communication, such as memory or a network interface.
- `tl_name`: This variable stores the name of the transport layer protocol being used, such as "TCP" or "UDP," to identify the type of transport being utilized.
- `tl_device`: This holds information about the specific device used for the transport layer, like a network card or other hardware, which handles the actual data transmission.
- `bitmap`: This variable is used to track and manage available or active resources, often in the form of a bitmask, where each bit represents the status of a specific resource.
- `iface`: This represents the communication interface used for data transfer. It stores information such as the memory domain, allocated worker, and other relevant details about the interface used for communication between layers.

## 3. Optimize System 
1. Below are the current configurations for OpenMPI and UCX in the system. Based on your learning, what methods can you use to optimize single-node performance by setting UCX environment variables?

```
-------------------------------------------------------------------
/opt/modulefiles/openmpi/ucx-pp:

module-whatis   {OpenMPI 4.1.6}
conflict        mpi
module          load ucx/1.15.0
prepend-path    PATH /opt/openmpi-4.1.6/bin
prepend-path    LD_LIBRARY_PATH /opt/openmpi-4.1.6/lib
prepend-path    MANPATH /opt/openmpi-4.1.6/share/man
prepend-path    CPATH /opt/openmpi-4.1.6/include
setenv          UCX_TLS ud_verbs
setenv          UCX_NET_DEVICES ibp3s0:1
-------------------------------------------------------------------
```

1. Please use the following commands to test different data sizes for latency and bandwidth, to verify your ideas:
```bash
module load openmpi/ucx-pp
mpiucx -n 2 $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_latency
mpiucx -n 2 $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_bw
```
- Becauses the current configuration is using `ud_verbs` only as the `UCX_TLS`, I guess there will be some flexibility and performance restriction when it is used on some applications. Therefore I try to set the `UCX_TLS` value to `all`, where every transports available are enabled and UCX can choose the most suitable one depending on the applications. The results are as follows:
#### UCX_TLS=ud_verbs (before)
- When the `UCX_TLS` environment variable is set to `ud_verbs`, UCX restricts itself to using only the `ud_verbs` transport layer for communication. This layer is primarily based on the InfiniBand UD protocol, using the Verbs interface. In the test output, the transport layer `ud_verbs/ibp3s0:1` is repeatedly selected for both intra-node and self configurations.
    #### osu_latency
    ```
    mpiucx -n 2 $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_latency
    UCX_TLS=ud_verbs
    0x55c5efac54a0 self cfg#0 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55bcfd37c3f0 self cfg#0 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55bcfd37c3f0 intra-node cfg#1 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55c5efac54a0 intra-node cfg#1 tag(ud_verbs/ibp3s0:1)
    ```
    #### osu_bw
    ```
    mpiucx -n 2 $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_bw
    UCX_TLS=ud_verbs
    0x55625c231410 self cfg#0 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55c86af723f0 self cfg#0 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55c86af723f0 intra-node cfg#1 tag(ud_verbs/ibp3s0:1)
    UCX_TLS=ud_verbs
    0x55625c231410 intra-node cfg#1 tag(ud_verbs/ibp3s0:1)
    ```
#### UCX_TLS=all (after)
- When the `UCX_TLS` environment variable is set to `all`, UCX is enabled to use `all` available transport layers. This allows UCX to dynamically select from a variety of available transport layers, such as `self/memory`, `cma/memory`, and `sysv/memory`, depending on the communication pattern and the resources available.
- Used Transport Layers
    - `self/memory`: This transport uses memory-based communication for processes running on the same node.
    - `cma/memory`: This transport layer allows memory-based communication over the `cma` (Communication Manager) API, typically used for intra-node communication.
    - `sysv/memory`: This transport uses memory mapping and shared memory mechanisms for communication, typically for inter-process communication on the same node.
    #### osu_latency
    ```
    mpiucx -n 2 -x UCX_TLS=all $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_latency
    UCX_TLS=all
    0x562274c494a0 self cfg#0 tag(self/memory cma/memory)
    UCX_TLS=all
    0x555c362f73f0 self cfg#0 tag(self/memory cma/memory)
    UCX_TLS=all
    0x555c362f73f0 intra-node cfg#1 tag(sysv/memory cma/memory)
    UCX_TLS=all
    0x562274c494a0 intra-node cfg#1 tag(sysv/memory cma/memory)
    ```
    #### osu_bw
    ```
    mpiucx -n 2 -x UCX_TLS=all $HOME/UCX-lsalab/test/mpi/osu/pt2pt/osu_bw
    UCX_TLS=all
    0x5598f696f410 self cfg#0 tag(self/memory cma/memory)
    UCX_TLS=all
    0x5608ac0f1480 self cfg#0 tag(self/memory cma/memory)
    UCX_TLS=all
    0x5608ac0f1480 intra-node cfg#1 tag(sysv/memory cma/memory)
    UCX_TLS=all
    0x5598f696f410 intra-node cfg#1 tag(sysv/memory cma/memory)
    ```

2. Please create a chart to illustrate the impact of different parameter options on various data sizes and the effects of different testsuite.

![](https://imgur.com/FDo919O.png)

![](https://imgur.com/TB6FKPu.png)

- The above charts illustrate the impact of two different UCX TLS configurations (`ud_verbs` and `all`) on latency and bandwidth across various data sizes.

3. Based on the chart, explain the impact of different TLS implementations and hypothesize the possible reasons (references required).
- **Latency (us)**:
    - The latency generally increases with data size for both configurations.
    - `UCX_TLS=ud_verbs` shows a steady rise in latency with larger data sizes, starting at around 1.89 Âµs for size 0 and reaching up to over 2300 Âµs for size 4194304.
    - `UCX_TLS=all` configuration has lower latency, staying mostly around 0.2 Âµs for smaller sizes, but increases more sharply with larger sizes, reaching over 960 Âµs at size 4194304.
- **Bandwidth (MB/s)**:
    - The bandwidth increases with data size, with all showing consistently higher bandwidth across all sizes compared to `UCX_TLS=ud_verbs`.
    - `UCX_TLS=ud_verbs` starts at 1.49 MB/s for size 1 and gradually increases, peaking at 2046.32 MB/s for size 4194304.
    - `UCX_TLS=all` starts at a much higher value of 10.11 MB/s for size 1 and increases significantly, reaching up to 8062.98 MB/s at size 4194304.
- In summary, the `UCX_TLS=all` configuration generally results in lower latency but higher bandwidth across all sizes, while `UCX_TLS=ud_verbs` results in higher latency but lower bandwidth. The latency increases more gradually for `UCX_TLS=ud_verbs`, while bandwidth increases more sharply for `UCX_TLS=all`.
- One possible reason that I think of is because of TLS implementation characteristics. `UCX_TLS=ud_verbs`, which is an unreliable datagram uses a lower-level transport mechanism, which may prioritize simplicity and direct access to underlying hardware over more complex optimizations like flow control and congestion management. As a result, latency tends to be higher since the implementation is less sophisticated in optimizing the transport layer and resource allocation. This could explain the steeper rise in latency with increasing data sizes.
- `UCX_TLS=all`, on the other hand, enables additional transport layer protocols and optimizations, such as flow control, congestion control, and possibly multipath transport. These optimizations reduce latency, especially at smaller data sizes, as the network protocol can manage traffic more efficiently.

### Advanced Challenge: Multi-Node Testing

This challenge involves testing the performance across multiple nodes. You can accomplish this by utilizing the sbatch script provided below. The task includes creating tables and providing explanations based on your findings. Notably, Writing a comprehensive report on this exercise can earn you up to 5 additional points.

- For information on sbatch, refer to the documentation at [Slurm's sbatch page](https://slurm.schedmd.com/sbatch.html).
- To conduct multi-node testing, use the following command:
```
cd ~/UCX-lsalab/test/
sbatch run.batch
```

- I run both osu_latency and osu_bw benchmark on single node and multi-node (2 nodes) to observe the impact of number of nodes to performance. As can be seen below, the transport layer selected are different in intra-node and inter-node configuration. For single node, it chooses to utilize shared memory TLS (`sysv/memory`), while for multi-node it utilizes reliable transfer TLS (`rc_verbs`).

- **intra-node**
    ```
    /home/pp24/pp24s098/ucx-pp/lib/libucm.so.0:/home/pp24/pp24s098/ucx-pp/lib/libucs.so.0:/home/pp24/pp24s098/ucx-pp/lib/libuct.so.0:/home/pp24/pp24s098/ucx-pp/lib/libucp.so.0
    0x55a7032a37f0 self cfg#0 tag(self/memory cma/memory)
    0x55a7032a37f0 intra-node cfg#1 tag(sysv/memory cma/memory)
    0x55e698adabf0 self cfg#0 tag(self/memory cma/memory)
    0x55e698adabf0 intra-node cfg#1 tag(sysv/memory cma/memory)
    ```
- **inter-node**
    ```
    /home/pp24/pp24s098/ucx-pp/lib/libucm.so.0:/home/pp24/pp24s098/ucx-pp/lib/libucs.so.0:/home/pp24/pp24s098/ucx-pp/lib/libuct.so.0:/home/pp24/pp24s098/ucx-pp/lib/libucp.so.0
    0x55f913176660 self cfg#0 tag(self/memory cma/memory rc_verbs/ibp3s0:1)
    0x55f913176660 inter-node cfg#1 tag(rc_verbs/ibp3s0:1 tcp/ibp3s0)
    0x561750c1c7f0 self cfg#0 tag(self/memory cma/memory rc_verbs/ibp3s0:1)
    0x561750c1c7f0 inter-node cfg#1 tag(rc_verbs/ibp3s0:1 tcp/ibp3s0)
    ```

- Below is the latency and bandwidth difference charts between running the benchmark on single and multi node configuration.

    ![](https://imgur.com/6XJirEA.png)

    ![](https://imgur.com/5k4lfuK.png)
- **Latency (us)**:
    - The latency values observed for the single-node configuration are relatively lower. This is because the system is using shared memory (`sysv/memory`), which provides faster communication between processes that are within the same physical machine. When scaling to a multi-node setup (2 nodes), on the other hand, the latency increases significantly. This is expected as communication between different nodes uses remote direct memory access (RDMA) via `rc_verbs` and the InfiniBand network (via `ibp3s0:1`). The increased network overhead and the physical distance between nodes lead to higher latency
- **Bandwidth (MB/s)**:
    - Bandwidth for the single-node setup is higher because the communication occurs locally using shared memory, which provide much higher throughput than network-based communication. Since no network protocols or physical distances are involved, the system can transfer data much faster. As with the latency, the bandwidth decreases when moving to a multi-node configuration. The chart indicates that the bandwidth in the multi-node setup is lower, reflecting the slower transfer rates of RDMA over InfiniBand and TCP/IP compared to local shared memory.
- In summary, the single-node configuration performs better in terms of both latency and bandwidth due to the use of local shared memory and optimized transport layers.The multi-node configuration experiences higher latency and reduced bandwidth, mainly due to the use of network-based communication protocols like RDMA and the physical separation between nodes.
- Surprisingly, the latency and bandwidth comparison chart between the single-node and multi-node configurations looks very similar to the comparison between the UCX_TLS configurations (`ud_verbs` and `all`). I think this is because both illustrate how changes in transport layer selection affect communication performance as data size increases. In both cases, different transport layers are chosen based on the configuration: single-node and multi-node configurations select different transport layers, just as `ud_verbs` and `all` do.

## 4. Experience & Conclusion
1. What have you learned from this homework?
- Through this homework, I have a better understanding of the functions and transmission of each layer of UCX. By tracing the code I also learned more about the UCX content taught by the teacher in class. I also realized more about the need for good hardware-software matching algorithm. I also think that the structure of UCX architecture is pretty clear and tracing its implementation code is relatively easy to understand compared to other open-source code I've traced before.

2. How long did you spend on the assignment?
- Around 3 days.

3. Feedback (optional)
