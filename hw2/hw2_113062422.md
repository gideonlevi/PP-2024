# Mandelbrot Set Parallelization

## Name: Gideon Levi
## Student ID: 113062422

---

## 1. Implementation

### 1.1 Pthread Version - Master-Slave Scheme (Dynamic Partitioning)
In this version, the Mandelbrot Set computation is parallelized using Pthreads in a master-slave approach. Here’s a detailed breakdown of the implementation and how it addresses task distribution, synchronization, and scalability.

#### 1.1.1 Task Partitioning and Distribution:
- Row-wise Distribution:
    - Each thread is assigned to compute one row of the image at a time.
    - A shared global counter (`current_row`) tracks the next row to be processed. The counter is incremented by the master thread and accessed by all slave threads.
- Dynamic Load Balancing:
    - Since some parts of the Mandelbrot Set converge faster than others, row-wise distribution helps manage varying workloads dynamically.
    - A mutex lock (`pthread_mutex_t`) ensures that only one thread can access and increment `current_row` at a time, preventing race conditions.

#### 1.1.2 Thread Creation and Management
- The number of threads is determined by querying the available CPU cores (`CPU_COUNT`).
- Pthreads are used to create and launch worker threads. Each thread executes the mandelbrot_slave function, where it computes rows of the image until no more rows are available.
- All threads are joined at the end to ensure the main thread waits for their completion before writing the image to a PNG file.

#### 1.1.3 Computation Logic (Mandelbrot Set Iterations)
- For each pixel, the algorithm iteratively computes whether the corresponding complex number escapes a radius of 2. If it doesn’t escape after a set number of iterations, the pixel is considered part of the Mandelbrot Set.
- Pixel value storage: The number of iterations for each pixel is stored in the `image` buffer, with each thread updating the buffer for the rows it processes.

#### Synchronization Using Mutex
- To avoid race conditions when accessing the shared row counter (`current_row`), a mutex lock is used.
- Each thread locks the counter, retrieves the next available row, increments the counter, and unlocks it. This ensures mutual exclusion during critical section access.

#### Image Generation
- Once all threads complete their computations, the main thrad writes the result to a PNG file using the `write_png` function. THis function encodes the `image` buffer into PNG format.

### 1.1 Pthread Version - Thread-Parallel Scheme (Static Partitioning)

#### 1.1.1 Task Partitioning and Distribution:
In the thread-parallel scheme with static partitioning, the workload is divided evenly among threads in advance, rather than assigned dynamically during execution. This differs from the master-slave scheme in several key ways:

- Pre-assigned Row Blocks:
    - Each thread is statically assigned specific rows to compute, based on its thread ID. For example, thread 0 will process rows 0, `num_threads`, `2 * num_threads`, etc.
    - No need for a shared global counter or mutex locks, as threads do not compete for row assignments.
- No Dynamic Load Balancing:
    - Since all assignments are determined at the start, this scheme assumes that the workload is evenly distributed. If some areas of the Mandelbrot Set are computationally heavier (i.e., require more iterations), this may result in load imbalance and lower efficiency compared to the dynamic approach.

#### 1.1.2 Thread Creation and Management
- Similar to the master-slave scheme, the number of threads is determined by querying the available CPU cores using `CPU_COUNT`.
- Each thread receives its share of rows using round-robin scheduling:
`row = thread_id + k * num_threads`.
- Each thread executes the `mandelbrot_thread` function, where it processes only the rows assigned to it. Threads complete their work independently without any further coordination.

#### 1.1.3 Computation Logic (Mandelbrot Set Iterations)
- The core logic of the Mandelbrot computation remains the same: each pixel is iterated over to determine whether the corresponding complex number escapes a radius of 2 within the specified iterations.
- The image buffer is updated directly by the threads, with each thread only modifying the portion corresponding to the rows it was assigned. No synchronization is required since there are no overlapping row assignments.

#### 1.1.4 Synchronization
- No Mutex Locks Needed:
    - Since row assignments are static, there is no need for mutex-based synchronization between threads.
    - This improves performance by avoiding the overhead of acquiring and releasing locks.

#### Image Generation
- Once all threads complete their assigned computations, the main thread collects the results and writes the image to a PNG file using the `write_png` function.

### Summary of Differences
Master-Slave Scheme assigns worklaod dynamically (based on global counter), which requires mutex for shared counter.	
Thread-Parallel Scheme assigsn workload staticly (pre-assigned rows to each thread), which do not require mutex.

Thread-Parallel scheme have no dynamic adjustment, which generates risk of workload imbalance. Master-Slace scheme introduce due to mutex lock synchronization.


### 1.2 Hybrid Version - Master-Slave Scheme (Dynamic Partitioning)
This section describes the hybrid parallel implementation of the Mandelbrot Set computation, using MPI and OpenMP. The master-slave scheme is applied to dynamically manage task distribution between processes, while OpenMP facilitates shared-memory parallelism within each MPI process. Below is a detailed breakdown of the design, task distribution, communication strategy, and potential issues.

#### 1.2.1 Design Overview:
- MPI (Inter-Process Communication):
    - Master-slave model distributes rows of the image among different MPI processes.
    - The master process assigns tasks dynamically to worker processes to handle non-uniform workloads efficiently.
    - Each worker process sends back the computed results to the master for final assembly.
- OpenMP (Intra-Process Parallelism):
    - Within each MPI process, OpenMP is used to further parallelize the computation of pixels in a row.
    - A `#pragma omp parallel for` directive ensures that multiple threads within a process can compute pixels concurrently.

#### 1.2.2 Task Partitioning and Distribution
- Dynamic Row Assignment via MPI:
    - The master process keeps track of the next available row to be processed using a `next_row` counter.
    - Initial rows are distributed to each MPI worker process. When a worker completes a row, the master either sends a new row or signals termination if all rows are processed.
    - This dynamic task allocation mitigates load imbalance, ensuring better scalability.
- Intra-Process Parallelization:
    - Each MPI process uses OpenMP to divide the pixel computation of its assigned rows.
    - OpenMP’s `schedule(dynamic)` ensures that threads handle portions of the row dynamically, improving load balance within the process.

#### 1.2.3 Communication Strategy
- Master Process:
    - Distributes rows to MPI worker processes using `MPI_Send()`.
    - Receives results from workers using `MPI_Recv()`.
    - Uses termination messages to signal workers when no more rows are available.
- Worker Process:
    - Receive rows from the master and compute pixel values using OpenMP.
    - Send the computed row back to the master via `MPI_Send()`.

#### 1.2.4 Synchronization
- Inter-Process Communication:
    - MPI ensures safe communication between processes, and there are no race conditions in accessing shared data between the master and worker processes.
- Intra-Process Synchronization:
    - OpenMP ensures that each thread computes only its assigned portion of the row without requiring explicit synchronization.

#### 1.2.5 Image Generation
- Once all rows are received, the master process assembles the image buffer and writes it to a PNG file using the `write_png()` function.


### 1.2 Hybrid Version - Thread Parallel Scheme (Static Partitioning)
In this implementation, each MPI process independently computes a portion of the image based on a static partitioning strategy, leveraging OpenMP for intra-process parallelism. The design, task distribution, communication strategy, and potential issues are outlined below.

#### 1.2.1 Design Overview:
- MPI (Inter-Process Communication):
    - Each MPI process is responsible for computing a specific set of rows from the image based on its rank.
    - There is no central master process; all processes operate independently, resulting in a simpler communication pattern.
- OpenMP (Intra-Process Parallelism):
    - OpenMP is employed to parallelize the pixel computation within each MPI process.
    - The `#pragma omp parallel for` directive is utilized to enable concurrent processing of pixels in a row.

#### 1.2.2 Task Partitioning and Distribution:
- Static Row Assignment via MPI:
    - The total height of the image is divided among the available MPI processes based on their ranks.
    - Each process calculates its local rows using a simple arithmetic formula based on the total number of rows and the process rank.
    - This static partitioning method may lead to load imbalances if the computation time per row varies significantly.
- Intra-Process Parallelization:
    - Within each MPI process, OpenMP further divides the computation of pixels in its assigned rows, allowing multiple threads to handle pixel calculations concurrently.

#### 1.2.3 Communication Strategy
- Data Gathering:
    - After the computation, each process sends its computed results back to the root process using `MPI_Gatherv()`, which gathers variable-sized arrays from all processes.
    - The `displs` and `rcounts` arrays are computed to facilitate the correct gathering of pixel data from all processes into a single image buffer.

#### 1.2.4 Synchronization
- Inter-Process Communication
    - MPI handles communication between processes, ensuring data consistency and correctness during the gathering phase.
- Intra-Process Synchronization:
    - OpenMP manages thread execution within each MPI process, allowing threads to compute pixel values without explicit synchronization beyond the scope of each parallel region.

#### 1.2.5 Image Generation
- Upon completion of the pixel computations, the root process assembles the final image using the gathered pixel data and writes it to a PNG file using the `write_png()` function.

- This implementation streamlines the process by removing the overhead of dynamic task assignment and termination signaling found in the master-slave scheme, but at the cost of potential load imbalances.

---

## 2. Experiment & Analysis

### 2.1 Experimental Setup
- **System Specs**: The experiment results are all run on the provided QCT cluster.
- **Test Data**: I used testcase strict34 from the testcases provided in the homework. I used this testcase because it has the longest runtime out of all the testcases based when run sequentially, which I think may cause the effects of changing configurations (number processes, cpus) easier to observe.
  
### 2.2 Performance Metrics
I evaluate the program's performances on different number of processes and threads, and utilizing `clock_gettime(CLOCK_MONOTONIC)` I evaluate them with these metrics:

- **hw2a**: Total execution time (CPU Time).
- **hw2b**: Total execution time (CPU Time + Communication Time).

Because the IO in all programs (both implementations of hw2a and hw2b) are the same, which is the write_png(), it is ignored in this experiment.

### 2.3 Performance Analysis 
- **Time Profile**: In this section I profile the total time taken for hw2a and hw2b with both master-slave and thread parallel implementation.
    - hw2a: For hw2a, I profile them with 2 configurations (8 threads and 12 threads). Although the difference is not significant, my master-slave implementation is a little bit faster compared to my thread parallel implementation.

    ![](https://imgur.com/lgcaVTK.jpeg)
    ![](https://imgur.com/qLGULm3.jpeg)
    
    - hw2b: Similar to hw2a, I also profile hw2b with 2 configurations. In contrast to hw2a, hw2b performs better in my implementation of thread parallel by a quite noticable difference. My guess is my master-slave scheme implementation is still inneficient in communication which is a large overhead in this scheme.

    ![](https://imgur.com/UBP9RpS.jpeg)
    ![](https://imgur.com/8kE20nF.jpeg)

- **Strong Scalability Plot (Achieved Speedup vs. Ideal Speedup Plot)**
    Starting from this step to the following, I used the master-slave scheme for hw2a and thread parallel scheme for hw2b since they are faster in the time profile above.

    - hw2a (single process)

    ![](https://imgur.com/s2GtX1W.jpeg)

    - hw2b (multi-process)

    ![](https://imgur.com/LIhvC0L.jpeg)

---

## 3. Discussions

### 3.1 Scalability Discussion:
1. hw2a
    - Performance: The performance for single-process execution closely align with ideal performance, demonstrating near-perfect overlap until around 10 threads, where a slight drop in performance is observed.
    - Reasons for Performance:
        - Single Process Communication: Since the implementation utilizes a single process, the communication cost among threads is significantly lower compared to multi-process setups. This facilitates efficient thread cooperation.
        - Independent Output: Each thread writes to different locations in the image, minimizing synchronization overhead and enhancing performance.

2. hw2b: The multi process graph shows the performance scaling for a multi-process program with varying numbers of processes and threads per process. Each solid line represents the actual speedup achieved with a certain number of processes, while the dashed lines represent the ideal performance if perfect scaling were achieved.
    - Increase in Speedup: As the number of processes increases, the speedup factor increases, confirming that distributing the workload across multiple processes is beneficial. The same thing can also be said for the impact of thread count.
    - Thread Efficiency: The rate of speed increase per thread in a single process is greater than that observed when using multiple processes. It can be seen that with more processes, the achieved speedup increasingly deviates from the ideal speedup. This indicates that the overhead of inter-process communication impacts performance and highlights the efficiency of internal thread communication compared to inter-process communication.

### 3.2 Load Balancing Discussion: 
In this section, I observe the load balancing performance of both hw2a (single process) and hw2b (multi-process).

- hw2a: I ran hw2a with a configuration of 12 threads and profiled the wall time for each running thread. The table below shows the results for both the master-slave and thread-parallel schemes. The data reveals that the load balancing is performing effectively in both cases, as evidenced by the similarity in wall times across all threads. This indicates that the workload is evenly distributed, minimizing the performance differences between threads.

![](https://imgur.com/YTEJ4aD.jpeg)

- hw2b: Similarly, I run both master-slave and thread parallel implementations of hw2b using 8 processes and 12 threads per process configuration, and profiled the wall time of each running process. The tables below display the results, revealing that the total time taken by each rank (except rank 0, which includes `write_png`) shows more variation in the thread-parallel scheme. This indicates that the load balancing is more efficient in the master-slave scheme, where dynamic partitioning of workloads is applied. In contrast, the thread-parallel scheme uses static partitioning from the start, which likely leads to less balanced workloads across processes.

![](https://imgur.com/f3IWC4h.jpeg)

---

## 4. Optimization Strategies
  
### 4.1 Applied Optimizatiosns

**1. Static Partitioning of Jobs:** 
- Originally, for both pthread and hybrid implementations of static partition scheme (non master-slave), I partition the heights into equal portion of continuous chunks, and distributes them into each thread. However, this approach had issues because as can be seen in some examples of the output image, partitioning jobs into continuous chunks causes the workload to be significantly uneven. This imbalance worsened with larger data sizes, leading to delays as threads waited for the slowest thread to finish.

![Testcase 34 output](https://imgur.com/aAba3zm.jpeg)

- To mitigate the delays caused by previous problem, I try to a better way to partition jobs staticly. I observe that by the nature of mandelbrot equation, which can also be observed in the nature of the output images, adjacent rows seems to have similar workload, e.g. images below. To take advantage of this to partition jobs more evenly, I implemented an interval assignment for static partitioning.

![](https://imgur.com/Dsb7b0k.jpeg)
![](https://imgur.com/mJ0ix6z.jpeg)

**2. Dynamic Partitioning (Master-Slave):**
- In hybrid implementation I originally specialized a process to be a master process where it does not participate in calculating any rows, and only responsible for assigning workloads and receiving results. But later I realized that with this scheme the master process is idling too much which kind of wasting the computing resources. Thus I also made the master process to participate in calculations in addition to the assignment of jobs and results receiving tasks.

**3. SIMD Vectorization:**
- I implemented SIMD vectorization using AVX-512 instructions to optimize the Mandelbrot set computation, which involves heavy arithmetic operations. In the Mandelbrot iteration loop, I used AVX-512 vector intrinsics to perform parallel calculations for eight pixels in each iteration. This includes vectorized operations for multiplication, addition, and comparison to efficiently evaluate whether each point remains within the Mandelbrot set. This SIMD-enabled implementation significantly reduces execution time compared to a scalar version by exploiting the hardware parallelism of modern CPUs. However, it also introduces constraints, such as ensuring that the image width is divisible by 8 for optimal performance. For cases where the width is not a multiple of 8, remaining columns are handled separately using scalar code.

- With AVX-512 and a multi-threaded approach, this implementation achieves better utilization of CPU resources, as can be seen in the speedups below:

- hw2a speedup

    ![](https://imgur.com/YpkSaTc.jpeg)
    ![](https://imgur.com/L6XPK5u.jpeg)

- hw2b speedup

    ![](https://imgur.com/YfiljBy.jpeg)
    ![](https://imgur.com/VxeROWZ.jpeg)


**4. PNG Write Parallelization**
- hw2a:
    - By profiling the program using Nsight Systems below, I notice that there is still bottleneck in the program, that is the png write done by `write_png()` function. Because it writes the png only on 1 thread serially, all the threads can only wait until the thread that is responsible for png write to finish. Parallelizing png writes is not trivial due to restrictive rules of the PNG format. For instance, PNG data can only be written row by row from the last row, and the file must be opened and closed by the same process.

    ![](https://imgur.com/IgpQgbb.jpeg)

    - My initial approach was to parallelize the RGB computation of the image and assign a single thread to execute all `png_write_row()` calls. However, as the profile result illustrates, the improvement is not much because computing RGB values was significantly faster than the `png_write_row()` operations, leaving the bottleneck largely unresolved.
    
    ![](https://imgur.com/M5HS0Ts.jpeg)

    - In a second approach, I introduced a mutex lock and a condition variable to control row-by-row writing. Here, each thread, upon finishing row computations, would contend for the lock and proceed with writing only if it was its turn (decided by condition variable's value). This approach improved speed compared to the previous version, but contention issues led to spurious wake-ups and idle CPU time, as evidenced by small gaps in CPU utilization for each thread in the profile result below.

    ![](https://imgur.com/VdNZIUZ.jpeg)

    - To further optimize the parallelization of png writes, I tried to reduce these excessive thread contentions by delegating the condition variable to a single dedicated thread for `png_write_row()`, while the other threads focus only on image computation. This way, the contentions of conditional variables can be fully removed, and blocking of a row's png write can only happen when the computation of that row have not completed (condition variable wait). This reduced spurious wake-ups and improved CPU utilization, achieving a significant performance gain, as shown in the profiling results.
    
    ![](https://imgur.com/vL2ZxBU.jpeg)
    ![](https://imgur.com/caXYbEF.jpeg)

- hw2b:
    - For hw2b, I implemented the similar parallelization of png writes with mutex lock and condition variable, but the performance of the optimized version turns out to be still slower compared to the non master-slave (thread parallel) version of hw2b. I do not implement png writes parallelization on thread parallel scheme of hw2b, because I think to do overlapping between png writes and computatiotn of rows in hw2b, the other processes will have to periodically send their computed image rows to the process that is responsible to write to the png file. With this, the amount of communication will be similar to the amount of communication in master-slave scheme, but with worse load-balancing. Therefore I concluded that parallelizing `write_png()` within the thread-parallel scheme of hw2b would likely not enhance performance. The speedup gained from parallelizing PNG writes in the master-slave configuration and a comparison with wall time in the non-master-slave version are shown below.

    ![](https://imgur.com/jbSG1px.jpeg)


### 4.2 Potential Optimizations
1. For cases with large image width size, distributing work one row at a time may result in excessive overhead and inefficiencies. To address this, experimenting with different portions of row to send at a time might help. Guided scheduling might also help to identify scenarios where it outperforms dynamic scheduling. Guided scheduling offers finer-grained workload distribution by assigning smaller chunks, such as portions of a row, instead of entire rows at once. This strategy might have several potential benefits like improved load balancing and reduces synchronization overhead.

---

## Others
- **Chunk Size Optimization**
    In my dynamic partitioning implementation with a master-slave scheme, tasks are initially divided at the row level, which provides fine-grained control but also risks higher contention for critical sections (in the pthread version) and greater communication overhead (in the hybrid MPI+OpenMP version). To address this, I experimented with larger chunk sizes, where each unit of work consisted of multiple rows. The intent was to reduce synchronization or communication frequency and thus potentially enhance performance.
    - However, this adjustment showed no notable performance gains. The contention for critical sections in the pthread version and the communication overhead in the hybrid MPI+OpenMP version appear minimal under the current setup, meaning the finer partitioning is still efficient. Based on these findings, I decided to keep the row-level granularity in the final implementation.

- **Compile Flags**
    - To optimize wall-time performance, I experimented with various compilation flags, such as `-Ofast`, `-ffast-math`, and `-funroll-loops`. These flags did improve execution speed but at a significant cost to accuracy, as they relax floating-point precision and make other aggressive optimizations. As a result, while these flags showed potential for time reduction, I ultimately decided against using them in the final configuration to preserve accuracy and ensure reliable results.

- **Pthread Guided Scheduling**
    - I tried to implement a scheduling scheme similar to OpenMP’s guided scheduling. I do this by gradually reducing the chunk size of rows each thread processes. At first, each thread processes larger blocks of rows, allowing them to work quickly on larger sections when more rows remain. As more rows get processed, each thread takes on smaller chunks, keeping the load balanced and reducing contention on the shared atomic variable that tracks row progress. This “logarithmic decay” approach helps avoid too many threads accessing the atomic variable at the same time, improving overall performance by spreading work more evenly. 
    - Although there was a slight performance gain from reduced contention, the improvement was marginal. This is likely due to the added overhead from calculating the changing block sizes, which offset much of the benefit.
    - In the end, I decided not to use this guided scheduling in my final results. To parallelize the PNG writing more effectively, it proved more efficient to process rows one at a time. This allowed for smoother overlap between row computation and writing, reducing idle time in the write stage.

- **SIMD Vectorization**
    - I also experimented with vectorizing the remaining columns when the image width is not a multiple of 8, leveraging AVX-512's masking capabilities. However, the performance gains were minimal and, in some cases, even slightly worse. This is likely due to the small number of remaining columns, where the additional overhead from memory transfers and function calls for SIMD operations can outweigh any benefit. As a result, I opted not to use this optimization for the remaining columns, as handling them with scalar code proved more efficient.

    ![](https://imgur.com/4xb4dEq.jpeg)

## 6. Experiences / Conclusion
Similar to the first homework assignment, I found this task to be an enjoyable experience, particularly in designing the most efficient solution for the program. While there are still some potential optimizations that I haven't fully explored, I found this assignment not as hard as the first one, where bottlenecks were more frequent and required more effort to mitigate delays caused by overhead.

This assignment enhanced my understanding of how threads are parallelized within a program's process, deepening my appreciation for parallelism concepts. One of the hardest parts for me was parallelizing the PNG writes due to the restrictive rules of the PNG format. However despite the challenges, I am generally satisfied with my results in this assignment.