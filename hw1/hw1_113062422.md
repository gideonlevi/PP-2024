# Odd-Even Sort using MPI

## Name: 鄔培勇
## Student ID: 113062422

---

## 1. Implementation

### 1.1 Handling Arbitrary Number of Input Items and Processes

**1. Input Size and Distribution:** 
- The number of elements per process is computed using `floor(n (# of inputs) / size (# of processes))` to distribute as evenly as possible. If `n` is not perfectly divisible by `size`, the remainder elements are distributed among the first few processes. I implemenet this by giving processes with a rank smaller than remainder one extra element.
- Each process calculates its own `displacement` (the index in the file where it starts reading). This is dones using a combiantion of `rank`, `base_size`, and the remainder distribution logic. The displacement helps the processes to access onlyi their respective parts of the file.
- Based on `displacement` and `local_size`, each process reads its own portion of data from the input file.

### 1.2 Sorting Method
**1. Local Sorting**
- Each process first sorts the chunk of data it reads from the input file using a sorting algorithm.

**2. Odd-Even Phases**

- After local sorting, the program performs an odd-even transposition sort, which is a comparison-based, distributed sorting algorithm. It works by repeatedly comparing and exchanging elements between adjacent processes.
- **Even Phase:**
    - In the even phase, if a process has an even rank, it compares its largest element with the smallest element of the next process (rank + 1). If its largest element is larger, they exchange data (all elements in each process).
    - If a process has an odd rank, it compares its smallest element with the largest element of the previous process (rank - 1). If its smallest element is smaller, they exchange data (all elements in each process).
- **Odd Phase:**
    - In the odd phase, the logic is reversed. Odd-ranked processes now cmopare their largest element with the smallest element of the next process (rank + 1), and even-ranked processes compare their smallest element with the largest element of the previous process.
- **Data Merging:**
    - After exchanging data with next or previous process, each process merges the two chunks of data, its own chunk and the received chunk.
    - Even rank processes will take the smaller half of the merged data and odd rank processes will take the larger half on even phase. Similarly on odd phase, Odd rank processes will take the smaller half of merged data and even rank processes take the larger half.

**3. Sorting Completion:**
    - With each iteration, both odd and even phases of the algorithm are executed, enabling neighboring processes to exchange boundary elements. This gradually pushes larger values toward the end and smaller values toward the beginning of the distributed data. By repeating this sequence for a fixed number of iterations, equal to the number of processes, the algorithm naturally achieves full global sorting.
    - After the data are globally sorted, all processes write its sorted chunk to the output file at the correct displacement similar to read.

### 1.3 MPI Communication
**1. MPI_Sendrecv**
- This function is used to send data to one process while simultaneously receiving data from another process.
- I use this communication function to first compare boundary elements of each process and also to exchange chunks of data of each processes when needed.

**2. MPI_IO**
- I use MPI's file I/O functions (`MPI_File_open`, `MPI_File_read_at`, and `MPI_File_write_at`) to handle parallel reading and writing of data.
- With these parallel I/O functions each process can read its own chunk of data from the input file and write its sorted chunk to the output file at the correct displacement in parallel.

### 1.4 Other Efforts

**1. Edge Cases:** When `n < size`, there are more processes than elements. In such cases only the first `n` ranks of processes participate in the sorting, and the rest processes will handle zero elements and will essentially be skipped during the sorting phase.

**2. Merge Functions:** 
- `merge_sort_large_half`: This function efficiently merges two chunks of data by only selecting the largest `local_size` elements between the two chunks, instead of fully merging both arrays. The function compares the elements starting from the largest and continues until `local_size` elements are gathered, which corresponds to the larger half of the combined data. This approach effectively cuts down the merge time in half, as it skips processing elements that would be discarded. The result of this merge is stored in the temporary buffer `res_buff`, and this function is used when a process is merging with its previous process (rank - 1).
- `merge_sort_small_half`: Similarly, this function merges two chunks of data by selecting the smallest `local_size` elements, starting from the smallest elements and proceeding until `local_size` elements are gathered. This is used when a process is merging with its next process (rank + 1). By focusing only on the smaller half of the combined data, the function saves significant time compared to a full merge operation. The result is also stored in `res_buff`, and this function is used when a process is merging with its next process (rank + 1).

---

## 2. Experiment & Analysis

### 2.1 Experimental Setup
- **System Specs**: The experiment results are all run on the provided Apollo cluster.
- **Test Data**: I used testcase number 35 from the testcases provided in the homework. I used this testcase because it has the longest runtime out of all the testcases in hw1-judge, which I think may cause the effects of changing configurations (number of node, processes) easier to observe.
  
### 2.2 Performance Metrics
I evaluate the program's performances on different number of nodes and processes, and measure them with these metrics:

- **Total Time**: Total execution time (Wall time).
- **IO Time**: Time taken for input/output using MPI-IO (MPI_File_open, MPI_File_write_at, MPI_File_close).
- **Communication Time**: Time spent in MPI communication (MPI_Sendrecv).
- **CPU Time**: Total execution time excluding Communication and IO time.

### 2.3 Compilers and MPIs
- In this experiment, I tested different combinations of compilers (`gcc`, `icc`) and MPI implementations (`intelmpi`, `openmpi`) to determine which setup would be the fastest setup for running the `hw1-judge` program. Here’s what I found:

![Complete time profile](https://imgur.com/RERlUvZ.jpeg)

- These results suggest that the combination of `gcc` with `openmpi` is the most efficient, delivering the shortest elapsed time. It appears that `openmpi`, in my testing environment, handles the hw1-judge workload a lot more effectively than intelmpi. Additionally, `gcc` seems to work slightly better compared to `icc` on both MPI implementations.
- Based on this result, for the following experiments I will be using `gcc` and `openmpi` to ensure the most efficient performance.

### 2.4 Performance Analysis
- **Time Profile**:
    - This plot shows the runtime of different numbers of processes per node (grouped by nodes) I profiled with `MPI_Wtime()`, broken down into CPU Time, Communication Time, and Input/Output Time.
    - Insight: As the number of processes per node increases, the total time decreases, but this reduction is not linear. You can see diminishing returns after a certain number of processes, especially as more nodes are added. CPU time dominates initially, but communication and IO time become more significant as you increase the number of nodes and processes.

    ![Complete time profile](https://i.imgur.com/geEMJff.jpeg)

- **Runtime vs Total Number of Processes**
    - This plot demonstrates the relationship between the total number of processes and the total runtime.
    - Insight: As the total number of processes increases, the runtime decreases significantly initially, but the curve flattens out, indicating diminishing returns. This reflects the overhead of parallelization, such as communication and synchronization between processes.

    ![Runtime vs Number of processes](https://i.imgur.com/pNEzrr4.png)

- **Strong Scalability Plot**:
    - This plot shows how the speedup factor increases as the total number of processes increases.
    - Insight: While the initial speedup from parallelism is large, communication overhead between processes limits further scalability. This is reflected in plot where the speedup curve flattens out as communication overhead offsets the benefits of adding more processes. This suggests that after a certain number of processes, adding more provides less additional speedup due to overhead, such as communication time between processes.

    ![Achieved Scalability](https://i.imgur.com/7rn9J1h.png)

- **Achieved Speedup vs. Ideal Speedup Plot**
    - This plot compares the actual speedup with the ideal speedup as the number of processes increases.
    - Insight: The red line indicates the ideal linear speedup (where doubling the number of processes would halve the runtime), while the blue line shows the actual speedup. The actual speedup starts diverging from the ideal speedup as the number of processes increases, highlighting the inefficiencies introduced by factors like communication overhead and I/O bottlenecks.

    ![Achieved vs Ideal Scalability](https://i.imgur.com/DD34c2p.png)

- **Number of nodes vs Runtime**
    - In this observation, the performance of the program improves when the same total number of processes is distributed across more nodes. Specifically, increasing the number of nodes results in a faster execution time.
    - Despite a slight increase in communication time as the number of nodes increases (because of slower inter-node communication compared to intra-node communication), the overall runtime decreases due to significant reductions in CPU Time and I/O Time. This is because the resources (particularly CPU and I/O) are more effectively distributed across the nodes. With more nodes available, the workload per node decreases, enabling more efficient use of CPU and I/O resources.
    - This results suggest that for workloads with high CPU and I/O demand, scaling by increasing the number of nodes (rather than simply increasing the number of processes) provides better overall performance gains, making it an important consideration in distributed computing environments.

    ![Number of nodes vs Runtime 1](https://i.imgur.com/N5gQodh.png)
    ![Number of nodes vs Runtime 2](https://i.imgur.com/udskXWe.png)


---

## 3. Optimization Strategies
  
### 3.1 Applied Optimizations

**1. Hybrid Sorting Algorithm Selection**: 
For smaller chunk sizes (`local_size`), the standard `std::sort` is used, an efficient comparison-based algorithm suitable for small to moderate-sized datasets. However, for larger chunks, the program switches to Boost's `spreadsort` algorithm, a non-comparison-based sorting method similar to radix sort, and is specifically optimized for large datasets and performs faster than comparison-based sorts in such cases. This hybrid approach ensures that the sorting process remains efficient across a wide range of dataset sizes. This hybrid sorting algorithm selection improves the elapsed time of `hw1-judge` program by around 2 seconds.

**2. Swap Instead of Copy:** 
After merging the data using either the `merge_sort_large_half` or `merge_sort_small_half` functions, the merged result is stored in a temporary buffer called `res_buff`. Instead of copying the contents of `res_buff` back into the process's main data array, I use `std::swap` to simply swap the pointers between data and res_buff. This eliminates the need for copying the actual data between arrays, which can be particularly expensive when `local_size` is large. As can be seen in the figure below, using `swap` instead of `copy` reduces the CPU time significantly.

![](https://i.imgur.com/4UhbbAF.png)

**3. Centralized vs Distributed I/O:** 
Initially, I implemented the input distribution using centralized I/O, where only the process with rank 0 reads the entire dataset and then distributes chunks to other processes using `Scatterv`. After sorting, the sorted data from all processes is gathered back to rank 0 using `Gatherv`, which writes the output file. I opted for this approach initially, assuming that starting I/O on each process might introduce excessive overhead and result in inefficiencies. This method works well for small values of `n` and `local_size`, but it does not scale effectively as the dataset size grows.

To address the scalability issue, I implemented distributed I/O. In this approach, each process independently calculates its own `local_size` and `displacement`, reading its own chunk of data directly from the input file. After sorting, each process writes its sorted chunk back to the output file in parallel. While the performance difference is negligible for smaller datasets, distributed I/O demonstrates significantly better scalability for larger datasets, as it distributes both computation and I/O more evenly across processes. Below is the improvement achieved with distributed I/O on testcase 35:

![](https://i.imgur.com/1hQzGql.png)

**4. Remove Allreduce Communications:**
- In my initial implementation, an `MPI_Allreduce` operation was called after each iteration of the odd-even phases to verify if data was globally sorted across all processes. This collective operation, though useful in detecting the end condition, introduced considerable communication overhead, particularly in early iterations when the sorting was still actively progressing. The `MPI_Allreduce` call forced all processes to synchronize, leading to unnecessary idle time for processes that had already completed their sorting task or were waiting on others, as shown in the profiling results below using Nsight Systems:

| Rank 1 Before Optimization |Rank 2 Before Optimization |
|---------|---------|
| ![Rank 1 profile](https://i.imgur.com/wnNHevR.jpeg) | ![Rank 2 profile](https://i.imgur.com/KkhzTxs.jpeg) |

- After analyzing the odd-even sort algorithm, I observed that, in each iteration, an element could move up to two processes away from its original position. This insight led to a breakthrough in determining a reliable stopping condition without requiring frequent global checks. Specifically:
    - For an even number of processes, the worst-case scenario (e.g., smallest element at the far-right process or largest at the far-left) requires at most `(total number of processes / 2)` iterations, because on each iteration elements can shift up to 2 processes.
    - For an odd number of processes, the worst-case scenario requires `(total number of processes / 2) + 1` iterations since elements on the last process shift only up to one process per iteration.

- With this insight in mind, instead of performing the global sort check after every iteration, the program now can just iterates only enough times to cover these worst-case scenarios, ensuring that all elements are sorted globally without needing intermediate `MPI_Allreduce` checks. This optimization fully removes all `MPI_Allreduce` calls, as illustrated in the updated profiling results:

| Rank 1 After Optimization | Rank 2 After Optimization |
|---------|---------|
| ![Rank 1 profile](https://imgur.com/gVnJ3cQ.jpeg) | ![Rank 2 profile](https://imgur.com/gUyV162.jpeg) |

As shown, the number of MPI_Allreduce instances has been completely removed in the optimized version, compared to frequent calls in the original.

![](https://imgur.com/f34sDWr.png)

As a result, the overall communication time is reduced and CPU utilization is improved due to minimized blocking.

**5. Small optimizations:**
I also implemented some small and simple tweaks to the program to further improve the performance.
- Replace module operation (%) for odd/even rank check with bitwise operation (&).
- Cache rank + 1, rank - 1, size - 1, and local_size - 1 calculations to reduce repeated calculation.
- Calculate only own displacement and `local_size` of adjacent processes instead of all processes to reduce computation and memory allocation.
- Remove explicit thread synchronization when the MPI_Communication or MPI_IO already have implicit synchronization.
- Originally, I used 2 receive buffers (to receive from previous and next processes). But later I figured that at a single phase (odd/even) only 1 is used, so they both can reuse the same receive buffer.
- I experimented with changing the order of odd and even phase, and do the even phase first. I notice a little improvement in the execution time, and my guess is because it increases the chance for process 0  to participate earlier with the sorting. Below is the improvement achieved by applying all of these small optimizations:

![](https://i.imgur.com/ILlM5Yb.png)

As shown in the figure above, these small optimizations slightly reduces both CPU time and communication time.

### 3.2 Potential Optimizations
- One way to optimize the CPU performance of the program is to take advantage of vector instructions (SIMD), such as AVX (Advanced Vector Extensions) or SSE (Streaming SIMD Extensions), which allow a single instruction to operate on multiple data points simultaneously. This parallel processing capability can significantly improve performance for workloads that involve repetitive computations.

- Another optimization is asynchronous communication to overlap computation and communication across iterations. Instead of waiting for data transfer to complete, asynchronous methods like non-blocking send/receive (e.g., `MPI_Isend` and `MPI_Irecv`) allow communication to happen in the background while computation is on going. This reduces idle time, enabling smoother iteration flow and better CPU utilization, especially in distributed systems or high-latency environments.

---

## 4. Discussion

### 4.1 Comparison of I/O, CPU, Network Performance
Compare I/O, CPU, Network performance. Which is/are the bottleneck(s)? Why? How could it be improved?

- **CPU Bottleneck**:
    - CPU Time is the primary bottleneck, especially when running with fewer processes or fewer nodes. The workload is CPU-bound initially, but as more processes are added per node, the workload becomes better distributed, reducing CPU time.

- **Communication Time**:
    - Communication Time may become a more prominent bottleneck as you scale across more nodes, though it's not severe in this setup.

### 4.2 Scalability
Compare scalability. Does your program scale well? Why or why not? How can you achieve better scalability? You may discuss the two implementations separately or together.

- According to the graph, the speedup initially increases quickly as some more processes are added, indicating good scalability up to a certain point. However, as the number of processes are getting larger, the speedup begins to plateau, indicating diminishing returns. This suggests that adding more processes beyond this point does not provide significant additional performance improvement. This is because as more processes are added, especially across nodes, the time spent communicating between processes becomes more significant, reducing the benefit of adding more processes.

- Reduce communication overhead by optimizing the data exchange patterns between processes, or reduce shared resource contention might be a good way to achieve better scalability of this program.

---

### 5. Others

- **Compile Flags**
    - To optimize wall-time performance, I experimented with various compilation flags, such as -Ofast, -ffast-math, and -funroll-loops. These flags did improve execution speed but at a significant cost to accuracy, as they relax floating-point precision and make other aggressive optimizations. As a result, while these flags showed potential for time reduction, I ultimately decided not to use them in the final configuration to preserve accuracy and ensure reliable results.

## 6. Experiences / Conclusion
This is my first time writing a parallel program, and I think it is a fun experience, especially designing and optimizing the program. It is easy to optimize at the beginning as you can easily find out bottleneck and redundant parts, and the runtime can be easily reduced. However when the runtime becomes smaller, it is not easy to optimize the program. In many cases, just changing some small places will result in a change on the runtime, or the status of the cluster is not good at that time, which will also affect the performance of each test performance. But after I spent a lot of time optimizing it, in the end I am really satisfied with the results.
